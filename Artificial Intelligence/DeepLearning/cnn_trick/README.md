# 卷积神经网络调参

## 更多优化算法

随机梯度下降 (局部极值、Saddle Point) -> 动量梯度下降 

* 受初始学习率影响很大

* 每一个维度学习率一样

AdaGrad 算法

* 前期，regularizer 较小，放大梯度
* 后期，regularizer 较大，缩小梯度
* 梯度随训练次数降低
* 每个分量有不同学习率

* 学习率太大，导致regularizer 影响敏感，导致梯度爆炸
* 后期，regularizer 累积值太大，提前结束训练

RMSProp

* Adagrad 变种
* 由累积平方梯度变为平均平方梯度
* 解决了后期提前结束的问题

Adam

* 动量梯度下降，稳定
* AdaGrad / RMSProp 自动调整梯度变化
* 三者结合 -> Adam

* 初始学习率比较小，有效

总结：
* 对于稀疏数据，使用学习率可自适应方法
* SGD 通常训练时间更长，最终效果比较好，但需要好的初始化和 learning rate
* 需要训练较深较复杂的网络其需要快速收敛时，推荐 Adam
* Adagrad,RMSprop,Adam 相近，在相似情况下表现差不多

## 激活函数

### Sigmoid：

* 输入非常大或非常小时没有梯度

* 输出均值非 0，（第一层归一化，其他层不归一化，出现偏差，均值非0，对CNN 不友好）
*  EXP 计算复杂
*  梯度消失  dfx / dx = fx (1-fx)

### Tanh

* 依旧没有梯度
* 均值为0
* 计算复杂

### ReLU

* 梯度不会过小
* 计算简单
* 收敛快
* 均值非0
* Dead ReLU

### Leaky-ReLU

### ELU

* 均值接近0
* x < 0 时，计算量大

### Maxout

* ReLU 泛化版本
* 两套参数

总结：

* ReLU 需要小心设置 LR
* 不要使用 sigmoid
* 使用 Leaky ReLU、maxout、ELU
* tanh 期望不要太大

## 网络初始化 

* 全部为 0
  * 单层网络可以（x可以直接参与结果）
  * 多层网络会梯度消失
    * 链式法则 

* 如何分析初始化结果好不好？
  * 查看初始化后各层的激活值分布 

## 批归一化

## 数据增强

* 归一化
* 图像变换
  * 翻转、拉伸、裁剪、变形
* 色彩变换
  * 对比度、亮度
* 多尺度

## 更多调参技巧

* 拿到更多数据
* 给神经网络添加层次，由浅边深
* 紧跟最新进展，使用新方法
* 增大训练的迭代次数
* 尝试正则化 ||w||^2
* 更多GPU来加速训练
* 可视化工具检查中间状态
  * 损失
  * 梯度
  * 准确率
  * 学习率
* 在标准数据集上训练
* 在小数据集上过拟合
* 数据集分布平衡
* 使用预调整好的稳定模型结构
* Fine-tuning
  * 预训练好的网络结构上进行微调
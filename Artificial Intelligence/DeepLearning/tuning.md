# 激活函数

![激活函数]()

## Sigmoid

* 输入非常大或非常小时没有梯度

* 输出均值非 0

* Exp 计算复杂

* 梯度消失

## Tanh

* 输入非常大或非常小时没有梯度

* 输出均值为 0

* Exp 计算复杂

## ReLU(Rectified Linear Unit)

* 梯度不会过小

* 计算量小

* 收敛速度快

* 输出均值非 0

* Dead ReLU : 一个非常大的梯度流过神经元，不会再对数据有激活现象

    * Leaky-ReLU

    * ELU

    * Maxout(ReLU 的泛化版本)

        * 参数 double

## 选择激活函数

* ReLU : 需要小心设置 learning rate 

* 不要使用 Sigmoid

* 使用 Leaky ReLU 、maxout、ELU

* 可以试试 tanh, 但不要抱太大期望

# 网络初始化

* 全部为 0 

    * 单层网络可行

    * 多层网络可行

        * 链式法则

* 均值为 0，方差为 0.02 的正态分布初始化—— tanh

    * 高层均值为 0，没有梯度

* 均值为 0，方差为 1 的正态分布初始化—— tanh

    * 高层均值为 -1, 1，饱和

* Xavier-tanh

> W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)

# 批归一化

* 每个batch在每一层上都做归一化

* 为了确保归一化能够起到作用，另设两个参数来逆归一化

# 数据增强

* 归一化

* 图像变换

    * 翻转、拉伸、裁剪、变形

* 色彩变换

    * 对比度、亮度

* 多尺度

# 更多调参技巧

* 拿到更多数据

* 给神经网络添加层次

* 紧跟最新进展，使用新方法

* 增大训练的迭代次数

* 尝试正则化

* 使用更多的 GPU 来加速训练

* 可视化工具来检查中间状态

    * 损失

    * 梯度

    * 准确率

    * 学习率

* 在标准数据集上训练

* 在小数据集上过拟合(重要)

* 数据集分布平衡

* 使用预调整好的稳定模型结构

* Fine-tuning

    * 预训练好的网络结构上进行微调

# Tensorflow 可视化

* 指定面板上显示的变量

* 训练过程中将这些变量计算出来，输出到文件中

* 文件解析 ./tensorboard --logdir=dir




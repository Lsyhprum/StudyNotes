{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念\n",
    "\n",
    "**支持度**:\n",
    "$$support(A \\Rightarrow B)=P(A \\cup B)$$\n",
    "\n",
    "**置信度**:\n",
    "$$confidence(A \\Rightarrow B)=P(B | A)$$\n",
    "\n",
    "**频繁项集、闭项集**:\n",
    "\n",
    "项的集合称为**项集**，项集的出现频度简称为项集的**频度，支持度计数**。如果项集的相对支持度满足预定义的最小支持度阙值，则称为**频繁项集**。由于\n",
    "\n",
    "$$confidence(A \\Rightarrow B)=\\frac{support(A \\cup B)}{support(A)}=\\frac{support\\_count(A \\cup B)}{support\\_count(A)}$$\n",
    "\n",
    "因此挖掘关联规则问题归结为挖掘频繁项集。\n",
    "\n",
    "一般而言，关联规则的挖掘过程有两步:\n",
    "* 找出所有频繁项集\n",
    "* 由频繁项集产生强关联规则\n",
    "\n",
    "从大型数据集中挖掘频繁项集的主要挑战是挖掘常常产生大量满足最小支持度阙值的项集，这是因为如果一个项集是频繁的，则它的子集也是频繁的。频繁项集的总个数为\n",
    "\n",
    "$$C^1_n+C^2_n+...+C^n_n=2^n-1$$\n",
    "\n",
    "如果不存在真超项集 Y 使得 Y 与 X 在 D 中具有相同的支持度计数，项集 X 是数据集 D 中的**闭频繁项集**。如果 X 是一个频繁项集，而且 X 的任意一个超集都是非频繁的，则称 X 是**最大频繁项集**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 频繁项集挖掘方法\n",
    "\n",
    "### Apriori 算法\n",
    "\n",
    "Apriori 使用一种称为逐层搜索的迭代方法，使用频繁 k 项集探索频繁 k+1 项集，直到不能再找到频繁 n 项集。为了提高频繁项集逐层产生的效率，使用先验性质压缩搜索空间。\n",
    "\n",
    "**先验性质**:频繁项集的非空子集也一定是频繁的。\n",
    "\n",
    "![](http://oh1zr9i3e.bkt.clouddn.com/18-4-21/61309405.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------L1--------\n",
      "[frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]\n",
      "---------C2--------\n",
      "[[1, 2], [1, 3], [1, 4], [1, 5], [2, 3], [2, 4], [2, 5], [3, 4], [3, 5], [4, 5]]\n",
      "---------L2--------\n",
      "[frozenset({1, 2}), frozenset({1, 3}), frozenset({1, 5}), frozenset({2, 3}), frozenset({2, 4}), frozenset({2, 5})]\n",
      "---------C3--------\n",
      "[[1, 2, 3], [1, 2, 5], [1, 3, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5]]\n",
      "---------L3--------\n",
      "[frozenset({1, 2, 3}), frozenset({1, 2, 5})]\n",
      "---------C4--------\n",
      "[[1, 2, 3, 5]]\n",
      "---------L4--------\n",
      "[]\n",
      "{frozenset({1}): 6, frozenset({2}): 7, frozenset({3}): 6, frozenset({4}): 2, frozenset({5}): 2, frozenset({1, 2}): 4, frozenset({1, 3}): 4, frozenset({1, 5}): 2, frozenset({2, 3}): 4, frozenset({2, 4}): 2, frozenset({2, 5}): 2, frozenset({1, 2, 3}): 2, frozenset({1, 2, 5}): 2}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    return np.array([[1, 2, 5], [2, 4], [2, 3], [1, 2, 4], [1, 3], [2, 3], [1, 3], [1, 2, 3, 5], [1, 2, 3]])\n",
    "\n",
    "\n",
    "def create_c1(data):\n",
    "    temp = []\n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if [col] not in temp:\n",
    "                temp.append([col])\n",
    "    temp.sort(key=lambda x: x[0])\n",
    "    return map(frozenset, temp)\n",
    "\n",
    "\n",
    "def scan_d(d, c_k, min_support):\n",
    "    ss_cnt = {}\n",
    "    support_data = {}\n",
    "    for can in c_k:\n",
    "        for tid in d:\n",
    "            if can.issubset(tid):\n",
    "                if can not in ss_cnt:\n",
    "                    ss_cnt[can] = 1\n",
    "                else:\n",
    "                    ss_cnt[can] += 1\n",
    "\n",
    "    res_list = []\n",
    "    for key in ss_cnt:\n",
    "        if ss_cnt[key] >= min_support:\n",
    "            res_list.append(key)\n",
    "            support_data[key] = ss_cnt[key]\n",
    "\n",
    "    print(res_list)\n",
    "    return np.array(res_list), support_data\n",
    "\n",
    "\n",
    "def apriori_gen(l):\n",
    "    res_list = []\n",
    "    for i in range(len(l)):\n",
    "        value_i = list(l[i])\n",
    "        for j in range(i + 1, len(l)):\n",
    "            value_j = list(l[j])\n",
    "            if value_i[:-1] == value_j[:-1]:\n",
    "                temp = np.hstack((value_i, value_j[-1]))\n",
    "                res_list.append(sorted(temp))\n",
    "\n",
    "    print(res_list)\n",
    "    return map(frozenset, res_list)\n",
    "\n",
    "\n",
    "def apriori(d, min_support):\n",
    "    ck = create_c1(d)\n",
    "    print(\"---------L1--------\")\n",
    "    lk, support_data = scan_d(d, ck, min_support)\n",
    "    k = 2\n",
    "    L = []\n",
    "\n",
    "    while len(lk) > 0:\n",
    "        L.append(lk)\n",
    "        print(\"---------C%d--------\" % k)\n",
    "        ck = apriori_gen(lk)\n",
    "        print(\"---------L%d--------\" % k)\n",
    "        lk, sup_data = scan_d(d, ck, min_support)\n",
    "        support_data.update(sup_data)\n",
    "        k += 1\n",
    "\n",
    "    return L, support_data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data = load_data()\n",
    "    l, support_data = apriori(test_data, 2)\n",
    "    print(support_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由频繁项集产生关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({5}) ----> frozenset({1})   conf=1.000000\n",
      "frozenset({4}) ----> frozenset({2})   conf=1.000000\n",
      "frozenset({5}) ----> frozenset({2})   conf=1.000000\n",
      "frozenset({2, 5}) ----> frozenset({1})   conf=1.000000\n",
      "frozenset({1, 5}) ----> frozenset({2})   conf=1.000000\n",
      "frozenset({5}) ----> frozenset({1, 2})   conf=1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    return np.array([[1, 2, 5], [2, 4], [2, 3], [1, 2, 4], [1, 3], [2, 3], [1, 3], [1, 2, 3, 5], [1, 2, 3]])\n",
    "\n",
    "\n",
    "def create_c1(data):\n",
    "    temp = []\n",
    "    for row in data:\n",
    "        for col in row:\n",
    "            if [col] not in temp:\n",
    "                temp.append([col])\n",
    "    temp.sort(key=lambda x: x[0])\n",
    "    return map(frozenset, temp)\n",
    "\n",
    "\n",
    "def scan_d(d, c_k, min_support):\n",
    "    ss_cnt = {}\n",
    "    sup_data = {}\n",
    "    for can in c_k:\n",
    "        for tid in d:\n",
    "            if can.issubset(tid):\n",
    "                if can not in ss_cnt:\n",
    "                    ss_cnt[can] = 1\n",
    "                else:\n",
    "                    ss_cnt[can] += 1\n",
    "\n",
    "    res_list = []\n",
    "    for key in ss_cnt:\n",
    "        if ss_cnt[key] >= min_support:\n",
    "            res_list.append(key)\n",
    "            sup_data[key] = ss_cnt[key]\n",
    "\n",
    "    return np.array(res_list), sup_data\n",
    "\n",
    "\n",
    "def apriori_gen(l):\n",
    "    res_list = []\n",
    "    for i in range(len(l)):\n",
    "        value_i = list(l[i])\n",
    "        for j in range(i + 1, len(l)):\n",
    "            value_j = list(l[j])\n",
    "            if value_i[:-1] == value_j[:-1]:\n",
    "                temp = np.hstack((value_i, value_j[-1]))\n",
    "                res_list.append(sorted(temp))\n",
    "    return map(frozenset, res_list)\n",
    "\n",
    "\n",
    "def apriori(d, min_support):\n",
    "    ck = create_c1(d)\n",
    "    lk, support_data = scan_d(d, ck, min_support)\n",
    "    k = 2\n",
    "    L = []\n",
    "\n",
    "    while len(lk) > 0:\n",
    "        L.append(lk)\n",
    "        ck = apriori_gen(lk)\n",
    "        lk, sup_data = scan_d(d, ck, min_support)\n",
    "        support_data.update(sup_data)\n",
    "        k += 1\n",
    "\n",
    "    return L, support_data\n",
    "\n",
    "\n",
    "def generate_rules(l, sup_data, min_conf=0.7):\n",
    "    rule_list = []\n",
    "    for i in range(1, len(l)):\n",
    "        for freq_set in l[i]:\n",
    "            h1 = [frozenset([item]) for item in freq_set]\n",
    "            if i > 1:\n",
    "                rules_conseq(freq_set, h1, sup_data, rule_list, min_conf)\n",
    "            else:\n",
    "                calc_conf(freq_set, h1, sup_data, rule_list, min_conf)\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "def calc_conf(freq, h, sup_data, rl, min_conf):\n",
    "    pruned_h = []\n",
    "\n",
    "    for conseq in h:\n",
    "        conf = sup_data[freq] / sup_data[freq - conseq]\n",
    "        if conf >= min_conf:\n",
    "            print(freq - conseq, \"---->\", conseq, end='')\n",
    "            print('   conf=%f' % conf)\n",
    "            rl.append((freq-conseq, conseq, conf))\n",
    "            pruned_h.append(conseq)\n",
    "    return pruned_h\n",
    "\n",
    "\n",
    "def rules_conseq(freq, h, sup_data, rl, min_conf):\n",
    "    m = len(h[0])\n",
    "    if len(freq) > (m + 1):\n",
    "        calc_conf(freq, h, sup_data, rl, min_conf)\n",
    "        hmp1 = apriori_gen(h)\n",
    "        hmp1 = calc_conf(freq, hmp1, sup_data, rl, min_conf)\n",
    "        if len(hmp1) > 1:\n",
    "            rules_conseq(freq, hmp1, sup_data, rl, min_conf)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data = load_data()\n",
    "    l, support_data = apriori(test_data, 2)\n",
    "    rule = generate_rules(l, support_data)\n",
    "    #print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# 感知机

## 导读

- 在[CH07](../CH07/README.md)中有说明， `分离超平面将特征空间划分为两个部分，一部分是正类， 一部分是负类。法向量指向的一侧为正类，另一侧为负类。`

## 模型

输入空间：$\mathcal X\sube \bf R^n$

输出空间：$\mathcal Y={+1,-1}$

> 决策函数：$f(x)=sign (w\cdot x+b)$

## 策略

确定学习策略就是定义**(经验)**损失函数并将损失函数最小化。

注意这里提到了**经验**，所以学习是base在**训练数据集**上的操作

#### 损失函数选择

> 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数**不是参数$w,b$的连续可导函数，不易优化**
>
> 损失函数的另一个选择是误分类点到超平面$S$的总距离，这是感知机所采用的

感知机学习的经验风险函数(损失函数)
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$
其中$M$是误分类点的集合

给定训练数据集$T$，损失函数$L(w,b)$是$w$和$b$的连续可导函数

### 算法

#### 原始形式

> 输入：$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\\ x_i\in \cal X=\bf R^n\mit , y_i\in \cal Y\it =\{-1,+1\}, i=1,2,\dots,N; \ \ 0<\eta\leqslant 1$
>
> 输出：$w,b;f(x)=sign(w\cdot x+b)$
>
> 1. 选取初值$w_0,b_0$
>
> 1. 训练集中选取数据$(x_i,y_i)$
>
> 1. 如果$y_i(w\cdot x_i+b)\leqslant 0$
>    $$
>    w\leftarrow w+\eta y_ix_i \nonumber\\
>    b\leftarrow b+\eta y_i
>    $$
> 4. 转至(2)，直至训练集中没有误分类点

注意这个原始形式中的迭代公式，可以对$x​$补1，将$w​$和$b​$合并在一起，合在一起的这个叫做扩充权重向量，书上有提到。

#### 对偶形式

对偶形式的基本思想是将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。
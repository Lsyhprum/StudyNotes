{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升方法\n",
    "\n",
    "提升方法是一种常用的统计学习方法，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "### 基本思路\n",
    "\n",
    "在概率近似正确(PAC)学习的框架中：\n",
    "\n",
    "**强可学习**：如果一个概念存在一个多项式学习算法能够学习它，且正确率很高，则称这个概念是强可学习的。\n",
    "\n",
    "**弱可学习**：如果一个概念存在一个多项式学习算法能够学习它，且正确率仅比随机猜测的效果略好，则称这个概念为弱可学习的。\n",
    "\n",
    "且强可学习与弱可学习是等价的，即在 PAC 学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。\n",
    "\n",
    "提升方法就是从弱学习方法出发，反复学习，得到一系列弱分类器，然后将这些弱分类器组合为强分类器。大多数提升方法是改变训练数据的概率分布，针对不同训练数据分布调用弱学习算法学习一系列弱分类器。\n",
    "\n",
    "AdaBoost 算法正是将*弱学习算法*提升为*强学习算法*的代表。\n",
    "\n",
    "### AdaBoost 算法\n",
    "\n",
    "**输入**:训练数据集\n",
    "\n",
    "$$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$$\n",
    "\n",
    "其中 \n",
    "\n",
    "$$x_i \\in X = R^n,y_i \\in Y={-1,1},i=1,2,...,N$$\n",
    "\n",
    "弱学习算法；\n",
    "\n",
    "**输出**:最终分类器G(x)\n",
    "\n",
    "* 初始化训练数据的权值分布:\n",
    "\n",
    "$$D_1=(w_11,..,w_1i,...,w_1N),w_{1i}=\\frac{1}{N}, i=1,2,..,N$$\n",
    "\n",
    "* 对 m = 1, 2,...,M\n",
    "\n",
    "    * 适应具有权值分布 D_m 的训练数据集学习，得到基本分类器\n",
    "    \n",
    "    $$G_m(x):X \\to {-1,+1}$$\n",
    "    \n",
    "    * 计算 G_m(x) 在训练数据集上的分类误差率( I(x) 为指示函数):\n",
    "    \n",
    "    $$e_m = \\sum^N_{i=1}P(G_m(x_i)\\ne y_i) = \\sum^N_{i=1}w_{mi}I(G_m(x_i) \\ne y_i)$$\n",
    "    \n",
    "    * 计算 G_m(x) 的系数(此处为自然对数)\n",
    "    \n",
    "    $$a_m= \\frac{1}{2}log{\\frac{1-e_m}{e_m}}$$\n",
    "    \n",
    "    * 更新训练数据集的权值分布\n",
    "    \n",
    "    $$D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})$$\n",
    "    \n",
    "    $$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-a_m y_i G_m(x_i)),i=1,2,...,N$$\n",
    "    \n",
    "    这里 Z_m 为规范因子\n",
    "    \n",
    "    $$Z_m = \\sum^N_{i=1}{w_{mi}}exp(-a_m y_i G_m(x_i))$$\n",
    "    \n",
    "    它使 D_{m+1} 成为一个概率分布\n",
    "    \n",
    "* 构建基本分类器的线性组合\n",
    "\n",
    "$$f(x)=\\sum^M_{m=1}a_mG_m(x)$$\n",
    "\n",
    "得到最终分类器\n",
    "\n",
    "$$G(x)=sign(f(x))=sign(\\sum^M_{m=1}a_m G_m(x))$$\n",
    "    \n",
    "**说明**:\n",
    "\n",
    "* 分类误差率:\n",
    "\n",
    "$$em=\\sum^N_{i=1}P(G_m(x_i)\\ne y_i) = \\sum_{G_m(x_i) \\ne y_i} w_{mi}$$\n",
    "\n",
    "这里 w_{mi} 表示第 m 轮中第 i 个实例的权值，\n",
    "\n",
    "$$\\sum^N_{i=1}w_{mi} = 1$$\n",
    "\n",
    "即分类误差率是被 G_m(x) 误分类样本的权值之和。\n",
    "\n",
    "* a_m 表示 G_m(x) 在最终分类器中的重要性，当\n",
    "\n",
    "$$e_m \\le \\frac{1}{2}$$\n",
    "\n",
    "$$a_m \\ge 0$$\n",
    "\n",
    "且 a_m 随 e_m 的减小而增大，所以**误分类率越小的基本分类器在最终分类器中作用越大**。\n",
    "\n",
    "* 更新训练数据权值分布公式也可写成\n",
    "\n",
    "$$w_{m+1,i}=\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{w_{mi}}{Z_m}exp(-a_m)&         &G_m(x_i)=y_i \\\\\n",
    "\\\\\n",
    "\\frac{w_{mi}}{Z_m}exp(a_m)&        &G_m(x_i)\\ne y_i \\\\\n",
    "\\end{aligned}\n",
    "\\right.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向分步算法\n",
    "\n",
    "AdaBoost 算法还有另一种解释，即可以认为 AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。\n",
    "\n",
    "考虑加法模型:\n",
    "    \n",
    "$$f(x)=\\sum^M_{m=1}β_mb(x;γ_m)$$\n",
    "\n",
    "其中 $$b(x;γ_m)$$ 为基函数，γ_m 为基函数的参数，β_m 为基函数的系数，显然为一个加法模型。\n",
    "\n",
    "给定训练数据及损失函数 L(y,f(x)) 的条件下，学习加法模型 f(x) 成为经验风险极小化问题:\n",
    "    \n",
    "$$min_{{γ_m},{β_m}}\\sum^N_{i=1}L(y_i,\\sum^N_{m=1}β_mb(x_i;γ_m))$$\n",
    "\n",
    "这通常是一个复杂优化问题，因为其为加法模型，从前向后，每一步只学习一个基函数及系数，逐渐逼近目标函数式，就可以简化优化的复杂度，即，每步只需优化如下损失函数:\n",
    "\n",
    "$$min_{β,γ}\\sum^N_{i=1}L(y_i,βb(x_i,γ))$$\n",
    "\n",
    "**输入**:\n",
    "训练数据集\n",
    "\n",
    "$$T=\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$$\n",
    "\n",
    "损失函数 L(y, f(x));基函数集{b(x;γ)};\n",
    "\n",
    "**输出**:\n",
    "加法模型 f(x)\n",
    "* 初始化 f_0(x) = 0\n",
    "* 对 m = 1...M\n",
    "    * 极小化损失函数 $$(β_m,γ_m)=argmin_{β,γ}\\sum^N_{i=1}L(y_i,f_{m-1}(x_i) + βb(x_i,γ))$$ 得到参数 β_m,γ_m\n",
    "    * 更新 $$f_m(x)=f_{m-1} + β_mb(x;γ_m)$$\n",
    "* 得到加法模型\n",
    "$$f(x) = f_M(x)=\\sum^M_{m=1}β_mb(x;γ_m)$$\n",
    "\n",
    "使用前向分步算法与 AdaBoost 算法样本权值更新只相差规范化因子，因而等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提升树\n",
    "\n",
    "提升树是以分类树或回归树为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一。\n",
    "\n",
    "## 提升树模型\n",
    "\n",
    "其基本分类器可以看作由一个根节点直接连接两个叶结点的简单决策树，即决策树桩，提升树模型可以表示为决策树的加法模型：\n",
    "\n",
    "$$f_M(x)=\\sum^M_{m=1}T(x;θ_m)$$\n",
    "\n",
    "其中 T(x;θ_m) 表示决策树，θ_m 为决策树参数，M 为树的个数。\n",
    "\n",
    "### 分类问题的提升树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f89e29b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_sim_data():\n",
    "    dataMat = np.matrix([[1, 2.1],\n",
    "                        [2., 1.1],\n",
    "                        [1.3, 1.],\n",
    "                        [1., 1.],\n",
    "                        [2., 1.]])\n",
    "    classLabels = [1., 1., -1., -1., 1.]\n",
    "    return dataMat, classLabels\n",
    "\n",
    "def draw_data(data, labels):\n",
    "    x = np.array(data[:, 0].transpose())[0]\n",
    "    y = np.array(data[:, 1].transpose())[0]\n",
    "    x_1 = []\n",
    "    y_1 = []\n",
    "    x_2 = []\n",
    "    y_2 = []\n",
    "    for i in range(0, len(labels)):\n",
    "        if labels[i] == 1:\n",
    "            x_1.append(x[i])\n",
    "            y_1.append(y[i])\n",
    "        else:\n",
    "            x_2.append(x[i])\n",
    "            y_2.append(y[i])\n",
    "    plt.scatter(x_1, y_1, marker='s', c='r')\n",
    "    plt.scatter(x_2, y_2)\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data, labels = load_sim_data()\n",
    "    draw_data(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestStump: {'dim': 0, 'thresh': 1.3, 'ineq': 'lt'}\n",
      "bestClasEst: [[-1.  1. -1. -1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    \"\"\"\n",
    "    使用只有一层的树桩决策树对数据分类\n",
    "    :param dataMatrix: 数据\n",
    "    :param dimen: 特征的下标\n",
    "    :param threshVal: 阙值\n",
    "    :param threshIneq: 大于或小于\n",
    "    :return: 分类结果\n",
    "    \"\"\"\n",
    "    retArray = np.ones((np.shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    \"\"\"\n",
    "    构建决策树\n",
    "    :param dataArr: 数据\n",
    "    :param classLabels: 标签\n",
    "    :param D: 训练数据权重\n",
    "    :return: 最佳决策树、最小错误率加权和、最优预测结果\n",
    "    \"\"\"\n",
    "    dataMatrix = np.mat(dataArr)\n",
    "    labelMat = np.mat(classLabels).T\n",
    "    m, n = np.shape(dataMatrix)\n",
    "    numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClasEst = np.mat(np.zeros((m, 1)))\n",
    "    minError = np.inf\n",
    "    for i in range(n):    # 遍历所有特征\n",
    "        rangeMin = dataMatrix[:, i].min()\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps    # 计算步长\n",
    "        for j in range(-1, int(numSteps) + 1):     # 遍历区间\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)   # 阙值\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)   # 调用树桩决策树分类\n",
    "                errArr = np.mat(np.ones((m, 1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T * errArr\n",
    "                #print(\"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:   # 记录最优树桩决策树\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    dataMat, classLabels = load_sim_data()\n",
    "    D = np.mat(np.ones((5, 1)) / 5)\n",
    "    bestStump, minError, bestClasEst = buildStump(dataMat, classLabels, D)\n",
    "    print(\"bestStump:\", bestStump)\n",
    "    print(\"bestClasEst:\", bestClasEst.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题的提升树\n",
    "\n",
    "回归问题提升树使用前向分步算法:\n",
    "\n",
    "$$f_0(x)=0$$\n",
    "\n",
    "$$f_m(x)=f_{m+1}(x) + T(x;θ_m), m = 1, 2, ..., M$$\n",
    "\n",
    "$$f_M(x)=\\sum^M_{m=1}T(x;θ_m)$$\n",
    "\n",
    "$$\\hat{θ_m}=argmin_{θ_m}\\sum^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x;θ_m))$$\n",
    "\n",
    "使用平方误差损失函数时:\n",
    "\n",
    "$$L(y,f_{m-1}(x)+T(x;θ_m)=[y-f_{m-1}(x)-T(x;θ_m)]^2=[r-T(x;θ_m)]^2$$\n",
    "\n",
    "r 为当前模型拟合数据的残差。所以对于回归问题的提升树算法只需简单拟合当前模型的残差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost 算法的训练误差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考**:\n",
    "* 统计学习方法\n",
    "* 机器学习实战"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
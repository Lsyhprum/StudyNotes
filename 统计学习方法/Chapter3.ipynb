{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k近邻法\n",
    "\n",
    "\n",
    "k近邻法是一种基本 **分类与回归** 方法。\n",
    "\n",
    "分类时，对新的实例根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。\n",
    "\n",
    "## k近邻算法\n",
    "\n",
    "**输入**:训练数据集 $$T = \\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N) \\}$$\n",
    "**输出**:实例 x 所属的类 y\n",
    "* 根据给定的距离度量，在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 k 个点的 x 的邻域记作 Nk(x)\n",
    "* 在 Nk(x) 中根据分类决策规则决定 x 的类别 y:\n",
    "\n",
    "$$y = argmax_{c_j}\\sum_{x_i\\in N_k(x)} I(y_i = c_j), i =1,2,...,N;j=1,2,...,K$$\n",
    "\n",
    "I 为指示函数，即当 $y_i = c_i$ 时 I 为 1，否则 I 为 0。\n",
    "\n",
    "k近邻不具有显式的学习过程。\n",
    "\n",
    "## k近邻模型\n",
    "\n",
    "模型由三个基本要素——**k 值的选择，距离度量和分类决策规则**决定。\n",
    "\n",
    "### 模型\n",
    "\n",
    "k近邻法使用的模型实际上对应于对 **特征空间的划分**。\n",
    "\n",
    "特征空间中对每个实例点 xi,距离该点比其他点更近的所有点组成一个区域，叫做单元(cell),所有单元构成对特征空间的一个划分。\n",
    "\n",
    "![](.\\pic\\6cbb8645jw1eoxamudrvgj20bb0a4gml.jpg)\n",
    "\n",
    "### 距离度量\n",
    "\n",
    "特征空间中两个实例点具距离是两个实例点相似程度的反应。\n",
    "\n",
    "设特征空间是 n 维实数向量空间 R^n,\n",
    "$$x_i=(x^{(1)}_i,x^{(2)}_i,...,x^{(n)}_i)^T$$\n",
    "$$x_j=(x^{(1)}_j,x^{(2)}_j,...,x^{(n)}_j)^T$$\n",
    "**闵可夫斯基距离 \\ Lp距离(Minkowski distance)**:\n",
    "\n",
    "$$L_p(x_i,x_j)=(\\sum^n_{l=1} \\vert x^{(l)}_i - x^{(j)}_j \\vert^p)^\\frac{1}{p}$$\n",
    "\n",
    "当 p = 2 时，称为**欧式距离**(Euclidean distance):\n",
    "\n",
    "$$L_2(x_i,x_j)=(\\sum^n_{l=1} \\vert x^{(l)}_i - x^{(j)}_j \\vert^2)^\\frac{1}{2}$$\n",
    "\n",
    "当 p = 1 时，称为**曼哈顿距离**(Manattan distance):\n",
    "\n",
    "$$L_1(x_i,x_j)=\\sum^n_{l=1} \\vert x^{(l)}_i - x^{(j)}_j \\vert$$\n",
    "\n",
    "L1 范数即在欧式空间固定直角坐标系上两点形成的线段对轴的投影距离总和\n",
    "\n",
    "当 p = ∞ 时，它是各个坐标距离的最大值:\n",
    "\n",
    "$$L_∞(x_i,x_j)=max_l \\vert x^{(l)}_i - x^{(j)}_j \\vert$$\n",
    "\n",
    "![](./pic/6cbb8645jw1eoxb5rzj1ej208q09a3yu.jpg)\n",
    "\n",
    "\n",
    "**推导**：\n",
    "\n",
    "\n",
    "![](./pic/20180120120335494.png)\n",
    "\n",
    "\n",
    "### k值的选择\n",
    "\n",
    "k较小，整体模型变得复杂，容易被噪声影响，发生过拟合。\n",
    "\n",
    "k较大，较远的训练实例也会对预测起作用，容易发生错误。\n",
    "\n",
    "### 分类决策规则\n",
    "\n",
    "多数表决规则：如果分类损失函数为 0-1 损失函数，分类函数为:\n",
    "\n",
    "$$f:R^n\\to\\{c_1,c2,..,c_K\\}$$\n",
    "\n",
    "则误分类的概率为:\n",
    "\n",
    "$$P(Y \\ne f(X))=1-P(Y=f(X))$$\n",
    "\n",
    "则误分类率为:\n",
    "\n",
    "$$\\frac{1}{k}\\sum_{x_i \\in N_k(x)} I(y_i \\ne c_j)=1-\\frac{1}{k}\\sum_{x_i \\in N_k(x)} I(y_i = c_j)$$\n",
    "\n",
    "多数表决规则等价于经验风险最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kd树\n",
    "\n",
    "算法核心在于如何快速搜素 k 近邻，朴素做法是线性扫描，不可取，这里采用 kd 树。\n",
    "\n",
    "### 构造平衡kd树\n",
    "\n",
    "对数据集T中的子集S初始化S=T，取当前节点node=root取维数的序数i=0，对S递归执行：\n",
    "\n",
    "找出S的第i维的中位数对应的点，通过该点，且垂直于第i维坐标轴做一个超平面。该点加入node的子节点。该超平面将空间分为两个部分，对这两个部分分别重复此操作（S=S'，++i，node=current），直到不可再分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, point):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.point = point\n",
    "\n",
    "def mid(lst):\n",
    "    m = len(lst) // 2\n",
    "    return lst[m], m\n",
    "\n",
    "def build_kdtree(data, d):\n",
    "    data = sorted(data, key=lambda x : x[d])\n",
    "    p, m = mid(data)\n",
    "    tree = Node(p)\n",
    "    \n",
    "    del data[m]\n",
    "\n",
    "    if m > 0:\n",
    "        tree.left = build_kdtree(data[:m], not d)\n",
    "    if len(data) > 1:\n",
    "        tree.right = build_kdtree(data[m:], not d)\n",
    "    return tree\n",
    "\n",
    "kd_tree = build_kdtree(T, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索kd树\n",
    "\n",
    "搜索跟二叉树一样，是一个递归的过程。先找到目标点的插入位置，然后往上走，逐步用自己到目标点的距离画个超球体，用超球体圈住的点来更新最近邻（或k最近邻）。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* KNN 是一种分类回归算法，通过对 k 个邻近点类别的多数表决进行分类\n",
    "\n",
    "* KNN 模型三要素：K 的值、距离度量、决策规则\n",
    "\n",
    "* KNN 算法：kd树，主要分为两步\n",
    "    * 构造 kd BST\n",
    "    * 回溯搜索 BST，通过候选超球与划分区域是否相交进行剪枝\n",
    "\n",
    "*参考*：\n",
    "\n",
    "[k近邻法](https://www.hankcs.com/ml/k-nearest-neighbor-method.html)\n",
    "\n",
    "[详解KDTree](https://blog.csdn.net/silangquan/article/details/41483689)\n",
    "\n",
    "[数据分析常识- 高纬度诅咒(curse of dimensionality)](https://zhuanlan.zhihu.com/p/23471291)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python36264bit36fc170516d04460ae12d1efa3e0666b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}